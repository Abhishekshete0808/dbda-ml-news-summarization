{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02eb876f-7bd5-4517-bfd2-d7e0646b1305",
   "metadata": {},
   "source": [
    "#### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefc3f0a-46b5-408b-827d-35edea1621e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    PegasusForConditionalGeneration,\n",
    "    PegasusTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78470e9e-4dac-4a46-9437-3bed6795b4e0",
   "metadata": {},
   "source": [
    "#### Set File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86065db1-6c73-4688-9f41-ec16cf73f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_TRAIN_PATH = r'..\\..\\Data\\Processed\\tokenized_pegasus_english\\train'\n",
    "TOKENIZED_TEST_PATH  = r'..\\..\\Data\\Processed\\tokenized_pegasus_english\\test'\n",
    "MODEL_OUTPUT_DIR     = r'..\\..\\Models\\pegasus_english_finetuned'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a39fa-ec68-4c48-ac97-936aafb6c9a1",
   "metadata": {},
   "source": [
    "#### Load Tokenized Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf38858-7929-4d81-81cd-8549778713ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 28225 | Test samples: 2889\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_from_disk(TOKENIZED_TRAIN_PATH)\n",
    "test_dataset  = load_from_disk(TOKENIZED_TEST_PATH)\n",
    "print(f\"Train samples: {len(train_dataset)} | Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a4b00-cb16-4b86-acd2-c0821e9e65c5",
   "metadata": {},
   "source": [
    "#### Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512881fa-6242-4e40-9ad3-55226c577c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882ba02-fb65-4448-acf3-66ea0cb298ef",
   "metadata": {},
   "source": [
    "#### Set Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e8af0d-6685-48ca-8fbf-20e4e9f4f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",         # Only evaluates after each epoch\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,                  # Keep only last 2 checkpoints\n",
    "    load_best_model_at_end=False,\n",
    "    logging_dir=os.path.join(MODEL_OUTPUT_DIR, \"logs\"),\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f968d-63fb-49c5-951f-c80de9d2c535",
   "metadata": {},
   "source": [
    "#### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932ebd0e-01e3-4bf7-911f-55f88888b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_17376\\2791099805.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934c7a3-dc79-45a0-9b0b-6b39b9c1e5dc",
   "metadata": {},
   "source": [
    "#### Resume from Latest Checkpoint If Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dea6a00-48ad-498e-ae88-75c0cba7c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ..\\..\\Models\\pegasus_english_finetuned\\checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1764' max='1765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1764/1765 32:12:25 < 01:31, 0.01 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.540100</td>\n",
       "      <td>0.768911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhi\\.conda\\envs\\pegasusenv\\lib\\site-packages\\transformers\\modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "checkpoints = sorted(glob.glob(os.path.join(MODEL_OUTPUT_DIR, \"checkpoint-*\")))\n",
    "if checkpoints:\n",
    "    print(f\"Resuming from checkpoint: {checkpoints[-1]}\")\n",
    "    trainer.train(resume_from_checkpoint=checkpoints[-1])\n",
    "else:\n",
    "    print(\"Starting training from scratch.\")\n",
    "    trainer.train()\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363b4a3f-ecb1-4a86-83dc-e7e00e147e77",
   "metadata": {},
   "source": [
    "#### Save Final Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a820b41c-6d33-4317-8b62-2264202398fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model and tokenizer saved to ..\\..\\Models\\pegasus_english_finetuned\\final_model\n"
     ]
    }
   ],
   "source": [
    "final_model_dir = os.path.join(MODEL_OUTPUT_DIR, \"final_model\")\n",
    "trainer.save_model(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "print(f\"Final model and tokenizer saved to {final_model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pegasusenv)",
   "language": "python",
   "name": "pegasusenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
