{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4db57d6-0b43-4398-b62e-161728510f07",
   "metadata": {},
   "source": [
    "#### Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef258fbe-3e71-4f9f-b3d9-a84f08731611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # For easier debugging (no effect on CPU)\n",
    "\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import html\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef63ed-ac2b-443c-a9f6-47a4fdf568a4",
   "metadata": {},
   "source": [
    "####  Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31dc273e-5cf6-4eb7-892d-ed46d43dfd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"../../Models/pegasus_english_finetuned/final_model\" \n",
    "RAW_TEST_PATH = \"../../Data/Processed/english_test_cleaned.csv\"           \n",
    "\n",
    "ARTICLE_COL = \"Article\"  \n",
    "SUMMARY_COL = \"Summary\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_SUMMARY_LENGTH = 100\n",
    "BEAM_SIZE = 4\n",
    "\n",
    "SAMPLE_SIZE = 300  # Number of clean & valid samples to evaluate\n",
    "MIN_ARTICLE_WORDS = 10\n",
    "MIN_SUMMARY_WORDS = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c665a3-7aea-49da-816d-350ff528e3da",
   "metadata": {},
   "source": [
    "#### Load Model, Tokenizer and Setup Device (CPU for stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb65016-919a-49c5-a12f-1f43b0c2f164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded model and tokenizer with vocab size: 96103\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(MODEL_DIR)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "assert tokenizer.vocab_size == model.config.vocab_size, \"Tokenizer and model vocab size mismatch!\"\n",
    "print(f\"Loaded model and tokenizer with vocab size: {tokenizer.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be588e-15e9-4ade-89e6-4217c94123fb",
   "metadata": {},
   "source": [
    "#### Load Raw Test Data and Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea77b82-0739-4422-84b0-c3df5775ac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw test samples loaded: 2889\n",
      "Samples after cleaning and dropping empty texts: 503\n",
      "Samples after applying min word length filter: 503\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(RAW_TEST_PATH)\n",
    "print(f\"Total raw test samples loaded: {len(test_df)}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize input text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = html.unescape(text)                      # decode HTML entities\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)   # zero-width characters\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)    # control chars\n",
    "    text = text.replace('#', ' ')                    # remove '#' symbol\n",
    "    disallowed_pattern = re.compile(r'[#$@%&*{}<>\\\\^~`|]')\n",
    "    # Early reject: drop rows with these disallowed special characters\n",
    "    if disallowed_pattern.search(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()           # normalize spaces\n",
    "    return text\n",
    "\n",
    "# Clean articles and summaries\n",
    "test_df[ARTICLE_COL] = test_df[ARTICLE_COL].astype(str).apply(clean_text)\n",
    "test_df[SUMMARY_COL] = test_df[SUMMARY_COL].astype(str).apply(clean_text)\n",
    "\n",
    "# Drop empty articles or summaries after cleaning\n",
    "test_df = test_df[(test_df[ARTICLE_COL] != \"\") & (test_df[SUMMARY_COL] != \"\")]\n",
    "print(f\"Samples after cleaning and dropping empty texts: {len(test_df)}\")\n",
    "\n",
    "# Filter by word length thresholds\n",
    "test_df['Article_len'] = test_df[ARTICLE_COL].apply(lambda x: len(x.split()))\n",
    "test_df['Summary_len'] = test_df[SUMMARY_COL].apply(lambda x: len(x.split()))\n",
    "test_df = test_df[\n",
    "    (test_df['Article_len'] >= MIN_ARTICLE_WORDS) & \n",
    "    (test_df['Summary_len'] >= MIN_SUMMARY_WORDS)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"Samples after applying min word length filter: {len(test_df)}\")\n",
    "\n",
    "# Drop auxiliary length columns\n",
    "test_df.drop(columns=['Article_len', 'Summary_len'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8abce61-daa0-4c7f-80b7-e1618241597d",
   "metadata": {},
   "source": [
    "#### Validate Token IDs and Randomly Select 300 Valid Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7fc247-99e5-4a8b-b23e-2d60842f712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering and validating samples by token ID range...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffe2927b0dc42eb9657e3cb381c7812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid samples found: 503\n",
      "Randomly selected 300 valid samples for evaluation.\n"
     ]
    }
   ],
   "source": [
    "valid_indices = []\n",
    "\n",
    "print(\"Filtering and validating samples by token ID range...\")\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    article_text = row[ARTICLE_COL]\n",
    "    inputs = tokenizer(\n",
    "        article_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "    )\n",
    "    max_id = inputs.input_ids.max().item()\n",
    "    min_id = inputs.input_ids.min().item()\n",
    "\n",
    "    if min_id < 0 or max_id >= tokenizer.vocab_size:\n",
    "        continue  # skip invalid samples\n",
    "\n",
    "    valid_indices.append(idx)\n",
    "\n",
    "print(f\"Total valid samples found: {len(valid_indices)}\")\n",
    "\n",
    "if len(valid_indices) < SAMPLE_SIZE:\n",
    "    print(f\"Warning: Only {len(valid_indices)} valid samples found, less than desired {SAMPLE_SIZE}\")\n",
    "\n",
    "# Random sampling for representative, random subset\n",
    "sampled_indices = random.sample(valid_indices, min(SAMPLE_SIZE, len(valid_indices)))\n",
    "eval_df = test_df.loc[sampled_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"Randomly selected {len(eval_df)} valid samples for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db17e4e0-c150-484a-9b6f-23f4d1e335e5",
   "metadata": {},
   "source": [
    "#### Inference on Clean Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6667be2-d1e4-453f-8463-b92b872e4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(\"Starting inference on selected clean samples...\")\n",
    "\n",
    "for idx, row in tqdm(eval_df.iterrows(), total=len(eval_df)):\n",
    "    article_text = row[ARTICLE_COL]\n",
    "    reference_summary = row[SUMMARY_COL]\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            article_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=MAX_SUMMARY_LENGTH,\n",
    "                num_beams=BEAM_SIZE,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {idx}: {e}\")\n",
    "        generated_summary = \"\"\n",
    "\n",
    "    results.append({\n",
    "        \"article\": article_text,\n",
    "        \"reference\": reference_summary,\n",
    "        \"generated\": generated_summary\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"pegasus_clean_filtered_eval_results.csv\", index=False)\n",
    "print(\"Saved inference results to 'pegasus_clean_filtered_eval_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdb5b7-b726-40eb-a532-13a4e5cdb499",
   "metadata": {},
   "source": [
    "#### Compute and Display ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53721ebc-cc3c-4c4b-9eed-385e88e13bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores on clean, randomly selected evaluation subset:\n",
      "ROUGE-1 F1: 0.2296\n",
      "ROUGE-2 F1: 0.1440\n",
      "ROUGE-L F1: 0.1996\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "predictions = results_df[\"generated\"].tolist()\n",
    "references = results_df[\"reference\"].tolist()\n",
    "\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "print(\"ROUGE Scores on clean, randomly selected evaluation subset:\")\n",
    "print(f\"ROUGE-1 F1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L F1: {rouge_scores['rougeL']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf40ed-e82a-422c-885b-4d25514003c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pegasusenv)",
   "language": "python",
   "name": "pegasusenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
